/**
 * å¤šæ¨¡æ€äº¤äº’å¼•æ“
 * æ”¯æŒè¯­éŸ³ã€å›¾ç‰‡ã€è¡¨æƒ…ã€æ‰‹åŠ¿ç­‰å¤šç§äº¤äº’æ–¹å¼
 */

interface VoiceConfig {
  enabled: boolean
  language: string
  pitch: number
  rate: number
  volume: number
  voiceURI?: string
}

interface VoiceMessage {
  id: string
  text: string
  audioData?: Blob
  duration?: number
  timestamp: number
}

interface ImageAnalysis {
  description: string
  emotions: string[]
  objects: string[]
  colors: string[]
  mood: string
  symbolism: string[]
}

interface GestureConfig {
  enabled: boolean
  sensitivity: number
  gestures: Map<string, string> // æ‰‹åŠ¿åç§° -> åŠ¨ä½œ
}

interface EmotionalExpression {
  type: 'emoji' | 'sticker' | 'animation'
  content: string
  intensity: number
  context: string
}

// è¯­éŸ³åˆæˆå’Œè¯†åˆ«ç®¡ç†å™¨
class VoiceManager {
  private synthesis: SpeechSynthesis
  private recognition: SpeechRecognition | null = null
  private config: VoiceConfig
  private isListening = false
  
  constructor() {
    this.synthesis = window.speechSynthesis
    this.config = {
      enabled: true,
      language: 'zh-CN',
      pitch: 1,
      rate: 0.9,
      volume: 0.8
    }
    this.initializeRecognition()
  }
  
  private initializeRecognition(): void {
    if ('webkitSpeechRecognition' in window) {
      this.recognition = new (window as any).webkitSpeechRecognition()
      this.recognition.continuous = false
      this.recognition.interimResults = false
      this.recognition.lang = this.config.language
    } else if ('SpeechRecognition' in window) {
      this.recognition = new SpeechRecognition()
      this.recognition.continuous = false
      this.recognition.interimResults = false
      this.recognition.lang = this.config.language
    }
  }
  
  // æ–‡æœ¬è½¬è¯­éŸ³
  async speak(text: string, deityId?: string): Promise<void> {
    if (!this.config.enabled) return
    
    return new Promise((resolve, reject) => {
      const utterance = new SpeechSynthesisUtterance(text)
      
      // æ ¹æ®ç¥ä»™é€‰æ‹©åˆé€‚çš„å£°éŸ³
      const voice = this.selectDeityVoice(deityId)
      if (voice) {
        utterance.voice = voice
      }
      
      utterance.pitch = this.config.pitch
      utterance.rate = this.config.rate
      utterance.volume = this.config.volume
      utterance.lang = this.config.language
      
      utterance.onend = () => resolve()
      utterance.onerror = (event) => reject(event.error)
      
      // åœæ­¢å½“å‰è¯­éŸ³
      this.synthesis.cancel()
      this.synthesis.speak(utterance)
    })
  }
  
  private selectDeityVoice(deityId?: string): SpeechSynthesisVoice | null {
    const voices = this.synthesis.getVoices()
    const chineseVoices = voices.filter(voice => voice.lang.startsWith('zh'))
    
    if (chineseVoices.length === 0) return null
    
    // æ ¹æ®ç¥ä»™ç‰¹æ€§é€‰æ‹©å£°éŸ³
    const voicePreferences = {
      'guanyin': ['female', 'gentle', 'soft'],
      'budongzun': ['male', 'strong', 'powerful'],
      'dashizhi': ['male', 'wise', 'deep'],
      'wenshu': ['male', 'scholarly', 'elegant'],
      'xukong': ['neutral', 'mysterious', 'ethereal'],
      'amitabha': ['male', 'calm', 'serene']
    }
    
    const preferences = voicePreferences[deityId as keyof typeof voicePreferences] || ['neutral']
    
    // ç®€å•çš„å£°éŸ³é€‰æ‹©é€»è¾‘
    if (preferences.includes('female')) {
      return chineseVoices.find(voice => voice.name.includes('Female')) || chineseVoices[0]
    } else if (preferences.includes('male')) {
      return chineseVoices.find(voice => voice.name.includes('Male')) || chineseVoices[0]
    }
    
    return chineseVoices[0]
  }
  
  // è¯­éŸ³è¯†åˆ«
  async startListening(): Promise<string> {
    if (!this.recognition || this.isListening) {
      throw new Error('è¯­éŸ³è¯†åˆ«ä¸å¯ç”¨æˆ–æ­£åœ¨è¿›è¡Œä¸­')
    }
    
    return new Promise((resolve, reject) => {
      this.isListening = true
      
      this.recognition!.onresult = (event) => {
        const transcript = event.results[0][0].transcript
        this.isListening = false
        resolve(transcript)
      }
      
      this.recognition!.onerror = (event) => {
        this.isListening = false
        reject(new Error(`è¯­éŸ³è¯†åˆ«é”™è¯¯: ${event.error}`))
      }
      
      this.recognition!.onend = () => {
        this.isListening = false
      }
      
      try {
        this.recognition!.start()
      } catch (error) {
        this.isListening = false
        reject(error)
      }
    })
  }
  
  stopListening(): void {
    if (this.recognition && this.isListening) {
      this.recognition.stop()
      this.isListening = false
    }
  }
  
  stopSpeaking(): void {
    this.synthesis.cancel()
  }
  
  updateConfig(newConfig: Partial<VoiceConfig>): void {
    this.config = { ...this.config, ...newConfig }
  }
  
  getConfig(): VoiceConfig {
    return { ...this.config }
  }
  
  isSupported(): boolean {
    return 'speechSynthesis' in window && (
      'webkitSpeechRecognition' in window || 'SpeechRecognition' in window
    )
  }
  
  getAvailableVoices(): SpeechSynthesisVoice[] {
    return this.synthesis.getVoices().filter(voice => voice.lang.startsWith('zh'))
  }
}

// å›¾åƒåˆ†æç®¡ç†å™¨
class ImageAnalyzer {
  // åˆ†æä¸Šä¼ çš„å›¾ç‰‡
  async analyzeImage(imageFile: File): Promise<ImageAnalysis> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader()
      
      reader.onload = async (event) => {
        const imageData = event.target?.result as string
        
        try {
          // åˆ›å»ºå›¾åƒå…ƒç´ è¿›è¡Œåˆ†æ
          const img = new Image()
          img.onload = () => {
            const analysis = this.performBasicAnalysis(img, imageData)
            resolve(analysis)
          }
          img.onerror = () => reject(new Error('å›¾ç‰‡åŠ è½½å¤±è´¥'))
          img.src = imageData
        } catch (error) {
          reject(error)
        }
      }
      
      reader.onerror = () => reject(new Error('æ–‡ä»¶è¯»å–å¤±è´¥'))
      reader.readAsDataURL(imageFile)
    })
  }
  
  private performBasicAnalysis(img: HTMLImageElement, imageData: string): ImageAnalysis {
    // åŸºç¡€å›¾åƒåˆ†æï¼ˆå®é™…äº§å“ä¸­ä¼šä½¿ç”¨æ›´é«˜çº§çš„AIè§†è§‰APIï¼‰
    const canvas = document.createElement('canvas')
    const ctx = canvas.getContext('2d')!
    
    canvas.width = img.width
    canvas.height = img.height
    ctx.drawImage(img, 0, 0)
    
    // è·å–å›¾åƒæ•°æ®
    const imagePixelData = ctx.getImageData(0, 0, canvas.width, canvas.height)
    
    // åˆ†æé¢œè‰²
    const colors = this.analyzeColors(imagePixelData)
    
    // åŸºç¡€æè¿°ç”Ÿæˆ
    const description = this.generateDescription(colors, img.width, img.height)
    
    // æƒ…æ„Ÿåˆ†æï¼ˆåŸºäºé¢œè‰²å’Œæ„å›¾çš„ç®€å•æ¨æ–­ï¼‰
    const emotions = this.inferEmotionsFromImage(colors)
    
    // å¿ƒæƒ…åˆ†æ
    const mood = this.analyzeMood(colors, emotions)
    
    return {
      description,
      emotions,
      objects: [], // ç®€åŒ–ç‰ˆæœ¬ä¸åšå¯¹è±¡è¯†åˆ«
      colors,
      mood,
      symbolism: this.analyzeSymbolism(colors)
    }
  }
  
  private analyzeColors(imageData: ImageData): string[] {
    const data = imageData.data
    const colorFrequency = new Map<string, number>()
    
    // é‡‡æ ·åˆ†æï¼ˆæ¯10ä¸ªåƒç´ é‡‡æ ·ä¸€æ¬¡ï¼‰
    for (let i = 0; i < data.length; i += 40) {
      const r = data[i]
      const g = data[i + 1]
      const b = data[i + 2]
      
      const colorName = this.getColorName(r, g, b)
      colorFrequency.set(colorName, (colorFrequency.get(colorName) || 0) + 1)
    }
    
    // è¿”å›å‡ºç°é¢‘ç‡æœ€é«˜çš„é¢œè‰²
    return Array.from(colorFrequency.entries())
      .sort((a, b) => b[1] - a[1])
      .slice(0, 5)
      .map(([color]) => color)
  }
  
  private getColorName(r: number, g: number, b: number): string {
    // ç®€åŒ–çš„é¢œè‰²åˆ†ç±»
    if (r > 200 && g > 200 && b > 200) return 'ç™½è‰²'
    if (r < 50 && g < 50 && b < 50) return 'é»‘è‰²'
    if (r > 150 && g < 100 && b < 100) return 'çº¢è‰²'
    if (r < 100 && g > 150 && b < 100) return 'ç»¿è‰²'
    if (r < 100 && g < 100 && b > 150) return 'è“è‰²'
    if (r > 150 && g > 150 && b < 100) return 'é»„è‰²'
    if (r > 150 && g < 100 && b > 150) return 'ç´«è‰²'
    if (r > 150 && g > 100 && b < 100) return 'æ©™è‰²'
    if (r > 100 && g > 100 && b > 100) return 'ç°è‰²'
    return 'å…¶ä»–'
  }
  
  private generateDescription(colors: string[], width: number, height: number): string {
    const aspect = width / height
    let composition = ''
    
    if (aspect > 1.5) composition = 'æ¨ªå‘æ„å›¾'
    else if (aspect < 0.7) composition = 'çºµå‘æ„å›¾'
    else composition = 'æ–¹å½¢æ„å›¾'
    
    const dominantColor = colors[0] || 'å¤šå½©'
    
    return `è¿™æ˜¯ä¸€å¼ ${composition}çš„å›¾ç‰‡ï¼Œä»¥${dominantColor}ä¸ºä¸»è‰²è°ƒï¼Œæ•´ä½“è‰²å½©${this.getColorMood(colors)}`
  }
  
  private getColorMood(colors: string[]): string {
    const warmColors = ['çº¢è‰²', 'æ©™è‰²', 'é»„è‰²']
    const coolColors = ['è“è‰²', 'ç»¿è‰²', 'ç´«è‰²']
    const neutralColors = ['ç™½è‰²', 'é»‘è‰²', 'ç°è‰²']
    
    const warmCount = colors.filter(c => warmColors.includes(c)).length
    const coolCount = colors.filter(c => coolColors.includes(c)).length
    const neutralCount = colors.filter(c => neutralColors.includes(c)).length
    
    if (warmCount > coolCount && warmCount > neutralCount) return 'æ¸©æš–'
    if (coolCount > warmCount && coolCount > neutralCount) return 'æ¸…å†·'
    if (neutralCount > 2) return 'ç´ é›…'
    return 'å’Œè°'
  }
  
  private inferEmotionsFromImage(colors: string[]): string[] {
    const emotions: string[] = []
    
    colors.forEach(color => {
      switch (color) {
        case 'çº¢è‰²':
          emotions.push('çƒ­æƒ…', 'æ´»åŠ›', 'å…´å¥‹')
          break
        case 'è“è‰²':
          emotions.push('å¹³é™', 'å®‰å®', 'æ·±æ²‰')
          break
        case 'ç»¿è‰²':
          emotions.push('è‡ªç„¶', 'å’Œè°', 'ç”Ÿæœº')
          break
        case 'é»„è‰²':
          emotions.push('å¿«ä¹', 'é˜³å…‰', 'å¸Œæœ›')
          break
        case 'ç´«è‰²':
          emotions.push('ç¥ç§˜', 'é«˜é›…', 'æµªæ¼«')
          break
        case 'é»‘è‰²':
          emotions.push('æ·±æ²‰', 'ä¸¥è‚ƒ', 'ç¥ç§˜')
          break
        case 'ç™½è‰²':
          emotions.push('çº¯å‡€', 'ç®€çº¦', 'å®é™')
          break
      }
    })
    
    return Array.from(new Set(emotions)).slice(0, 3)
  }
  
  private analyzeMood(colors: string[], emotions: string[]): string {
    if (emotions.includes('å¹³é™') || emotions.includes('å®é™')) return 'å®é™'
    if (emotions.includes('çƒ­æƒ…') || emotions.includes('å…´å¥‹')) return 'æ¿€æ˜‚'
    if (emotions.includes('å¿«ä¹') || emotions.includes('é˜³å…‰')) return 'æ„‰æ‚¦'
    if (emotions.includes('æ·±æ²‰') || emotions.includes('ç¥ç§˜')) return 'æ·±é‚ƒ'
    if (emotions.includes('è‡ªç„¶') || emotions.includes('å’Œè°')) return 'å’Œè°'
    return 'å¹³å’Œ'
  }
  
  private analyzeSymbolism(colors: string[]): string[] {
    const symbolism: string[] = []
    
    colors.forEach(color => {
      switch (color) {
        case 'çº¢è‰²':
          symbolism.push('å‰ç¥¥', 'å–œåº†', 'åŠ›é‡')
          break
        case 'é‡‘è‰²':
          symbolism.push('å¯Œè´µ', 'ç¥åœ£', 'æ™ºæ…§')
          break
        case 'ç´«è‰²':
          symbolism.push('è´µæ—', 'ç¥ç§˜', 'çµæ€§')
          break
        case 'ç™½è‰²':
          symbolism.push('çº¯æ´', 'ç¥åœ£', 'æ–°ç”Ÿ')
          break
        case 'ç»¿è‰²':
          symbolism.push('ç”Ÿå‘½', 'æˆé•¿', 'å¸Œæœ›')
          break
        case 'è“è‰²':
          symbolism.push('å¤©ç©º', 'æµ·æ´‹', 'æ— é™')
          break
      }
    })
    
    return Array.from(new Set(symbolism)).slice(0, 3)
  }
}

// æ‰‹åŠ¿è¯†åˆ«ç®¡ç†å™¨
class GestureManager {
  private config: GestureConfig
  private isTracking = false
  private gestureCallbacks = new Map<string, Function>()
  
  constructor() {
    this.config = {
      enabled: false, // é»˜è®¤å…³é—­ï¼Œéœ€è¦ç”¨æˆ·ä¸»åŠ¨å¯ç”¨
      sensitivity: 0.5,
      gestures: new Map([
        ['wave', 'æŒ¥æ‰‹é—®å€™'],
        ['thumbsUp', 'ç‚¹èµ'],
        ['peace', 'å’Œå¹³æ‰‹åŠ¿'],
        ['heart', 'å¿ƒå½¢æ‰‹åŠ¿'],
        ['pray', 'ç¥ˆç¥·æ‰‹åŠ¿']
      ])
    }
  }
  
  // å¯åŠ¨æ‰‹åŠ¿è¯†åˆ«ï¼ˆéœ€è¦æ‘„åƒå¤´æƒé™ï¼‰
  async startGestureTracking(): Promise<void> {
    if (!this.isSupported()) {
      throw new Error('å½“å‰è®¾å¤‡ä¸æ”¯æŒæ‰‹åŠ¿è¯†åˆ«')
    }
    
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true })
      this.isTracking = true
      
      // è¿™é‡Œåº”è¯¥é›†æˆæ‰‹åŠ¿è¯†åˆ«åº“ï¼Œå¦‚ MediaPipe æˆ– TensorFlow.js
      // ç®€åŒ–ç‰ˆæœ¬åªæä¾›æ¥å£
      console.log('æ‰‹åŠ¿è¯†åˆ«å·²å¯åŠ¨')
      
      return Promise.resolve()
    } catch (error) {
      throw new Error('æ— æ³•è®¿é—®æ‘„åƒå¤´')
    }
  }
  
  stopGestureTracking(): void {
    this.isTracking = false
    console.log('æ‰‹åŠ¿è¯†åˆ«å·²åœæ­¢')
  }
  
  // æ³¨å†Œæ‰‹åŠ¿å›è°ƒ
  onGesture(gesture: string, callback: Function): void {
    this.gestureCallbacks.set(gesture, callback)
  }
  
  // ç§»é™¤æ‰‹åŠ¿å›è°ƒ
  offGesture(gesture: string): void {
    this.gestureCallbacks.delete(gesture)
  }
  
  // æ¨¡æ‹Ÿæ‰‹åŠ¿è§¦å‘ï¼ˆç”¨äºæµ‹è¯•ï¼‰
  simulateGesture(gesture: string): void {
    const callback = this.gestureCallbacks.get(gesture)
    if (callback) {
      callback(gesture)
    }
  }
  
  isSupported(): boolean {
    return 'mediaDevices' in navigator && 'getUserMedia' in navigator.mediaDevices
  }
  
  getConfig(): GestureConfig {
    return { ...this.config }
  }
  
  updateConfig(newConfig: Partial<GestureConfig>): void {
    this.config = { ...this.config, ...newConfig }
  }
}

// æƒ…æ„Ÿè¡¨è¾¾ç®¡ç†å™¨
class EmotionalExpressionManager {
  private expressions = new Map<string, EmotionalExpression[]>()
  
  constructor() {
    this.initializeExpressions()
  }
  
  private initializeExpressions(): void {
    // åŸºç¡€è¡¨æƒ…åŒ…
    this.expressions.set('joy', [
      { type: 'emoji', content: 'ğŸ˜Š', intensity: 50, context: 'gentle_joy' },
      { type: 'emoji', content: 'ğŸ˜„', intensity: 70, context: 'strong_joy' },
      { type: 'emoji', content: 'ğŸ‰', intensity: 90, context: 'celebration' },
      { type: 'sticker', content: 'å¼€å¿ƒ.gif', intensity: 80, context: 'animated_joy' }
    ])
    
    this.expressions.set('sadness', [
      { type: 'emoji', content: 'ğŸ˜¢', intensity: 60, context: 'gentle_sadness' },
      { type: 'emoji', content: 'ğŸ˜­', intensity: 80, context: 'strong_sadness' },
      { type: 'sticker', content: 'å®‰æ…°.gif', intensity: 70, context: 'comfort' }
    ])
    
    this.expressions.set('anger', [
      { type: 'emoji', content: 'ğŸ˜¤', intensity: 60, context: 'mild_anger' },
      { type: 'emoji', content: 'ğŸ˜ ', intensity: 80, context: 'strong_anger' }
    ])
    
    this.expressions.set('surprise', [
      { type: 'emoji', content: 'ğŸ˜®', intensity: 60, context: 'mild_surprise' },
      { type: 'emoji', content: 'ğŸ˜±', intensity: 90, context: 'shock' }
    ])
    
    this.expressions.set('love', [
      { type: 'emoji', content: 'ğŸ’•', intensity: 70, context: 'gentle_love' },
      { type: 'emoji', content: 'â¤ï¸', intensity: 90, context: 'strong_love' },
      { type: 'sticker', content: 'çˆ±å¿ƒ.gif', intensity: 85, context: 'animated_love' }
    ])
    
    this.expressions.set('gratitude', [
      { type: 'emoji', content: 'ğŸ™', intensity: 70, context: 'thank_you' },
      { type: 'emoji', content: 'ğŸŠ', intensity: 80, context: 'celebration_thanks' }
    ])
    
    this.expressions.set('peace', [
      { type: 'emoji', content: 'â˜®ï¸', intensity: 60, context: 'peace_symbol' },
      { type: 'emoji', content: 'ğŸ•‰ï¸', intensity: 80, context: 'spiritual_peace' },
      { type: 'sticker', content: 'è²èŠ±.gif', intensity: 90, context: 'lotus_peace' }
    ])
  }
  
  // æ ¹æ®æƒ…æ„Ÿå’Œå¼ºåº¦é€‰æ‹©è¡¨è¾¾æ–¹å¼
  selectExpression(emotion: string, intensity: number = 50, context?: string): EmotionalExpression | null {
    const emotionExpressions = this.expressions.get(emotion.toLowerCase())
    if (!emotionExpressions) return null
    
    // ç­›é€‰åˆé€‚å¼ºåº¦çš„è¡¨è¾¾æ–¹å¼
    const suitableExpressions = emotionExpressions.filter(expr => {
      const intensityMatch = Math.abs(expr.intensity - intensity) < 30
      const contextMatch = !context || expr.context.includes(context)
      return intensityMatch && contextMatch
    })
    
    if (suitableExpressions.length === 0) {
      return emotionExpressions[0] // è¿”å›é»˜è®¤è¡¨è¾¾æ–¹å¼
    }
    
    // éšæœºé€‰æ‹©ä¸€ä¸ªåˆé€‚çš„è¡¨è¾¾æ–¹å¼
    return suitableExpressions[Math.floor(Math.random() * suitableExpressions.length)]
  }
  
  // è·å–æ‰€æœ‰å¯ç”¨çš„æƒ…æ„Ÿç±»å‹
  getAvailableEmotions(): string[] {
    return Array.from(this.expressions.keys())
  }
  
  // æ·»åŠ è‡ªå®šä¹‰è¡¨è¾¾æ–¹å¼
  addCustomExpression(emotion: string, expression: EmotionalExpression): void {
    if (!this.expressions.has(emotion)) {
      this.expressions.set(emotion, [])
    }
    this.expressions.get(emotion)!.push(expression)
  }
  
  // ç”Ÿæˆæƒ…æ„Ÿè¡¨è¾¾å»ºè®®
  suggestExpressions(userMessage: string, emotion: string, intensity: number): EmotionalExpression[] {
    const suggestions: EmotionalExpression[] = []
    
    // åŸºäºæ¶ˆæ¯å†…å®¹çš„è¡¨è¾¾å»ºè®®
    const messageExpressions = this.analyzeMessageForExpressions(userMessage)
    suggestions.push(...messageExpressions)
    
    // åŸºäºæƒ…æ„Ÿçš„è¡¨è¾¾å»ºè®®
    const emotionExpression = this.selectExpression(emotion, intensity)
    if (emotionExpression) {
      suggestions.push(emotionExpression)
    }
    
    // å»é‡å¹¶é™åˆ¶æ•°é‡
    const uniqueSuggestions = suggestions
      .filter((expr, index, self) => 
        index === self.findIndex(e => e.content === expr.content)
      )
      .slice(0, 4)
    
    return uniqueSuggestions
  }
  
  private analyzeMessageForExpressions(message: string): EmotionalExpression[] {
    const expressions: EmotionalExpression[] = []
    
    // å…³é”®è¯åŒ¹é…
    const keywords = {
      'è°¢è°¢': { type: 'emoji', content: 'ğŸ™', intensity: 70, context: 'gratitude' },
      'å¼€å¿ƒ': { type: 'emoji', content: 'ğŸ˜Š', intensity: 70, context: 'joy' },
      'éš¾è¿‡': { type: 'emoji', content: 'ğŸ˜¢', intensity: 60, context: 'sadness' },
      'ç”Ÿæ°”': { type: 'emoji', content: 'ğŸ˜¤', intensity: 70, context: 'anger' },
      'æƒŠè®¶': { type: 'emoji', content: 'ğŸ˜®', intensity: 60, context: 'surprise' },
      'çˆ±': { type: 'emoji', content: 'ğŸ’•', intensity: 80, context: 'love' },
      'ç¥ˆç¥·': { type: 'emoji', content: 'ğŸ™', intensity: 80, context: 'prayer' },
      'å¹³é™': { type: 'emoji', content: 'â˜®ï¸', intensity: 60, context: 'peace' }
    }
    
    Object.entries(keywords).forEach(([keyword, expr]) => {
      if (message.includes(keyword)) {
        expressions.push(expr as EmotionalExpression)
      }
    })
    
    return expressions
  }
}

// å¤šæ¨¡æ€äº¤äº’å¼•æ“ä¸»ç±»
export class MultimodalEngine {
  private voiceManager: VoiceManager
  private imageAnalyzer: ImageAnalyzer
  private gestureManager: GestureManager
  private emotionalExpressionManager: EmotionalExpressionManager
  private isInitialized = false
  
  constructor() {
    this.voiceManager = new VoiceManager()
    this.imageAnalyzer = new ImageAnalyzer()
    this.gestureManager = new GestureManager()
    this.emotionalExpressionManager = new EmotionalExpressionManager()
  }
  
  async initialize(): Promise<void> {
    if (this.isInitialized) return
    
    try {
      // æ£€æŸ¥å„ç§åŠŸèƒ½çš„æ”¯æŒæƒ…å†µ
      const capabilities = {
        voice: this.voiceManager.isSupported(),
        gesture: this.gestureManager.isSupported(),
        camera: 'mediaDevices' in navigator
      }
      
      console.log('å¤šæ¨¡æ€äº¤äº’å¼•æ“åˆå§‹åŒ–å®Œæˆ', capabilities)
      this.isInitialized = true
    } catch (error) {
      console.error('å¤šæ¨¡æ€äº¤äº’å¼•æ“åˆå§‹åŒ–å¤±è´¥:', error)
      throw error
    }
  }
  
  // è¯­éŸ³åŠŸèƒ½
  async speakMessage(message: string, deityId?: string): Promise<void> {
    return this.voiceManager.speak(message, deityId)
  }
  
  async startVoiceInput(): Promise<string> {
    return this.voiceManager.startListening()
  }
  
  stopVoiceInput(): void {
    this.voiceManager.stopListening()
  }
  
  stopSpeaking(): void {
    this.voiceManager.stopSpeaking()
  }
  
  // å›¾åƒåˆ†æåŠŸèƒ½
  async analyzeUploadedImage(file: File): Promise<ImageAnalysis> {
    return this.imageAnalyzer.analyzeImage(file)
  }
  
  // æ‰‹åŠ¿è¯†åˆ«åŠŸèƒ½
  async enableGestureRecognition(): Promise<void> {
    return this.gestureManager.startGestureTracking()
  }
  
  disableGestureRecognition(): void {
    this.gestureManager.stopGestureTracking()
  }
  
  onGesture(gesture: string, callback: (gesture: string) => void): void {
    this.gestureManager.onGesture(gesture, callback)
  }
  
  // æƒ…æ„Ÿè¡¨è¾¾åŠŸèƒ½
  suggestEmotionalExpressions(
    message: string, 
    emotion: string, 
    intensity: number = 50
  ): EmotionalExpression[] {
    return this.emotionalExpressionManager.suggestExpressions(message, emotion, intensity)
  }
  
  selectExpression(emotion: string, intensity: number = 50): EmotionalExpression | null {
    return this.emotionalExpressionManager.selectExpression(emotion, intensity)
  }
  
  // é…ç½®ç®¡ç†
  updateVoiceConfig(config: Partial<VoiceConfig>): void {
    this.voiceManager.updateConfig(config)
  }
  
  updateGestureConfig(config: Partial<GestureConfig>): void {
    this.gestureManager.updateConfig(config)
  }
  
  // åŠŸèƒ½æ”¯æŒæ£€æŸ¥
  getCapabilities() {
    return {
      voiceSupported: this.voiceManager.isSupported(),
      gestureSupported: this.gestureManager.isSupported(),
      cameraSupported: 'mediaDevices' in navigator,
      voiceConfig: this.voiceManager.getConfig(),
      gestureConfig: this.gestureManager.getConfig(),
      availableVoices: this.voiceManager.getAvailableVoices(),
      availableEmotions: this.emotionalExpressionManager.getAvailableEmotions()
    }
  }
  
  // æ™ºèƒ½äº¤äº’å»ºè®®
  generateInteractionSuggestions(context: {
    currentEmotion: string
    messageType: string
    userPreferences: any
  }): {
    voice: boolean
    gestures: string[]
    expressions: EmotionalExpression[]
    imageUpload: boolean
  } {
    const suggestions = {
      voice: false,
      gestures: [] as string[],
      expressions: [] as EmotionalExpression[],
      imageUpload: false
    }
    
    // åŸºäºæƒ…æ„ŸçŠ¶æ€çš„å»ºè®®
    if (context.currentEmotion === 'sadness') {
      suggestions.voice = true // å»ºè®®ä½¿ç”¨è¯­éŸ³ï¼Œæ›´æœ‰æ¸©åº¦
      suggestions.gestures.push('pray', 'heart')
      suggestions.expressions.push(
        ...this.emotionalExpressionManager.suggestExpressions('', 'sadness', 70)
      )
    } else if (context.currentEmotion === 'joy') {
      suggestions.gestures.push('thumbsUp', 'wave')
      suggestions.expressions.push(
        ...this.emotionalExpressionManager.suggestExpressions('', 'joy', 80)
      )
    }
    
    // åŸºäºæ¶ˆæ¯ç±»å‹çš„å»ºè®®
    if (context.messageType === 'question') {
      suggestions.imageUpload = true // å¯èƒ½éœ€è¦å±•ç¤ºç›¸å…³å›¾ç‰‡
    } else if (context.messageType === 'sharing') {
      suggestions.voice = true // åˆ†äº«å†…å®¹å»ºè®®è¯­éŸ³
      suggestions.imageUpload = true // å¯èƒ½æƒ³åˆ†äº«å›¾ç‰‡
    }
    
    return suggestions
  }
}

// å¯¼å‡ºå•ä¾‹å®ä¾‹
export const multimodalEngine = new MultimodalEngine()

// å¯¼å‡ºç±»å‹
export type { 
  VoiceConfig, 
  VoiceMessage, 
  ImageAnalysis, 
  GestureConfig, 
  EmotionalExpression 
}